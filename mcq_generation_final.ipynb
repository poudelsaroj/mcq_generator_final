{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmjSjaiGHeZ"
      },
      "source": [
        "# MCQ Generation with T5 Fine-Tuning and Classical Distractor Pipeline\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryxvKZxrGHef",
        "outputId": "3ba33083-2659-4453-877a-c5c821ee346d"
      },
      "source": [
        "!pip install spacy sense2vec requests nltk sentence-transformers transformers datasets --quiet\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m838.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf s2v_reddit_2015_md.tar.gz\n",
        "!mv s2v_reddit_2015_md s2v_old\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jan8bkVIMJWN",
        "outputId": "5af47c82-e54e-49da-ee29-e1671156b544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-25 16:39:12--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250225%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250225T163912Z&X-Amz-Expires=300&X-Amz-Signature=6403d3d3fc6d6e10b372b367cb08168dea61ba624d978c3b1fa10e6915f38876&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-25 16:39:12--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250225%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250225T163912Z&X-Amz-Expires=300&X-Amz-Signature=6403d3d3fc6d6e10b372b367cb08168dea61ba624d978c3b1fa10e6915f38876&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600444501 (573M) [application/octet-stream]\n",
            "Saving to: ‘s2v_reddit_2015_md.tar.gz’\n",
            "\n",
            "s2v_reddit_2015_md. 100%[===================>] 572.63M   115MB/s    in 5.1s    \n",
            "\n",
            "2025-02-25 16:39:18 (113 MB/s) - ‘s2v_reddit_2015_md.tar.gz’ saved [600444501/600444501]\n",
            "\n",
            "./._s2v_old\n",
            "./s2v_old/\n",
            "./s2v_old/._freqs.json\n",
            "./s2v_old/freqs.json\n",
            "./s2v_old/._vectors\n",
            "./s2v_old/vectors\n",
            "./s2v_old/._cfg\n",
            "./s2v_old/cfg\n",
            "./s2v_old/._strings.json\n",
            "./s2v_old/strings.json\n",
            "./s2v_old/._key2row\n",
            "./s2v_old/key2row\n",
            "mv: cannot stat 's2v_reddit_2015_md': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGA3Do_kGHeh"
      },
      "source": [
        "## 1. Imports & Basic Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nNVINVDGHei"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import random\n",
        "import requests\n",
        "import spacy\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sense2vec import Sense2Vec\n",
        "\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load SBERT for distractor re-ranking\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Attempt to load local Sense2Vec model\n",
        "try:\n",
        "    s2v = Sense2Vec().from_disk(\"s2v_old\")  # folder with sense2vec data\n",
        "except:\n",
        "    print(\"Sense2Vec model folder 's2v_old' not found. Distractor generation with sense2vec may be partial.\")\n",
        "    s2v = None\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3zkmEBUXHdC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0atWIcNGHej"
      },
      "source": [
        "## 2. Prepare RACE & SQuAD for T5\n",
        "\n",
        "We'll load **RACE** (all config) and **SQuAD** from Hugging Face, converting each sample into the format:\n",
        "```\n",
        "context => \"Generate MCQ: <passage>\"\n",
        "target  => \"Question: <question> Answer: <answer>\"\n",
        "```\n",
        "We'll combine them in a single JSONL file for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7519dnb7GHek"
      },
      "source": [
        "def process_race(split=\"train\"):\n",
        "    \"\"\"\n",
        "    Loads the RACE dataset (e.g., 'race', 'all'), uses the specified split.\n",
        "    Returns a list of dicts: {context, target, domain}.\n",
        "    \"\"\"\n",
        "    race_ds = load_dataset(\"race\", \"all\", split=split)\n",
        "    samples = []\n",
        "    for item in race_ds:\n",
        "        article = item.get(\"article\", \"\").strip()\n",
        "        question = item.get(\"question\", \"\").strip()\n",
        "        options = item.get(\"options\", [])\n",
        "        answer_field = item.get(\"answer\", None)\n",
        "\n",
        "        # If the answer is an integer index, map it to the actual option text\n",
        "        if isinstance(answer_field, int) and options:\n",
        "            correct_ans = options[answer_field].strip()\n",
        "        else:\n",
        "            if answer_field is not None:\n",
        "                correct_ans = str(answer_field).strip()\n",
        "            else:\n",
        "                correct_ans = \"\"\n",
        "\n",
        "        context_str = \"Generate MCQ: \" + article\n",
        "        target_str = f\"Question: {question} Answer: {correct_ans}\"\n",
        "\n",
        "        samples.append({\n",
        "            \"context\": context_str,\n",
        "            \"target\": target_str,\n",
        "            \"domain\": \"english\"\n",
        "        })\n",
        "    return samples\n",
        "\n",
        "def process_squad(split=\"train\"):\n",
        "    \"\"\"\n",
        "    Loads SQuAD from Hugging Face, uses the specified split.\n",
        "    Returns a list of dicts: {context, target, domain}.\n",
        "    \"\"\"\n",
        "    squad_ds = load_dataset(\"squad\", split=split)\n",
        "    samples = []\n",
        "    for item in squad_ds:\n",
        "        context_text = item.get(\"context\", \"\").strip()\n",
        "        question = item.get(\"question\", \"\").strip()\n",
        "        answers = item.get(\"answers\", {}).get(\"text\", [])\n",
        "        if answers:\n",
        "            answer_str = answers[0].strip()\n",
        "        else:\n",
        "            answer_str = \"\"\n",
        "\n",
        "        context_str = \"Generate MCQ: \" + context_text\n",
        "        target_str = f\"Question: {question} Answer: {answer_str}\"\n",
        "\n",
        "        samples.append({\n",
        "            \"context\": context_str,\n",
        "            \"target\": target_str,\n",
        "            \"domain\": \"english\"\n",
        "        })\n",
        "    return samples\n",
        "\n",
        "def combine_datasets(race_split=\"train[:2000]\", squad_split=\"train[:2000]\"):\n",
        "    \"\"\"\n",
        "    Combine samples from RACE and SQuAD.\n",
        "    Adjust the splits as desired.\n",
        "    \"\"\"\n",
        "    race_samples = process_race(split=race_split)\n",
        "    squad_samples = process_squad(split=squad_split)\n",
        "    combined = race_samples + squad_samples\n",
        "    return combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWZDV0IxGHek"
      },
      "source": [
        "### Create & Save the Unified JSONL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM9j7ZA9GHel"
      },
      "source": [
        "# Combine RACE + SQuAD (using slices for demo)\n",
        "all_samples = combine_datasets(\n",
        "    race_split=\"train[:20000]\",  # or 'train' to use the entire dataset\n",
        "    squad_split=\"train[:20000]\"   # or 'train' to use the entire dataset\n",
        ")\n",
        "\n",
        "output_file = \"/content/drive/MyDrive/MinorProject/unified_mcq_dataset.jsonl\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sample in all_samples:\n",
        "        f.write(json.dumps(sample) + \"\\n\")\n",
        "\n",
        "print(f\"Unified dataset saved to {output_file} with {len(all_samples)} samples.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXllavoRGHel"
      },
      "source": [
        "## 3. `MCQDataset` for Fine-Tuning\n",
        "We'll define a custom `Dataset` that:\n",
        "- Reads each JSON line,\n",
        "- Tokenizes `context` as the T5 **input**,\n",
        "- Tokenizes `target` as the T5 **labels**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYELzGyTGHem"
      },
      "source": [
        "class MCQDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        jsonl_file,\n",
        "        tokenizer,\n",
        "        max_source_length=512,\n",
        "        max_target_length=128\n",
        "    ):\n",
        "        self.samples = []\n",
        "        with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                self.samples.append(json.loads(line))\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_source_length = max_source_length\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        source_text = sample[\"context\"]\n",
        "        target_text = sample[\"target\"]\n",
        "\n",
        "        # Tokenize the source\n",
        "        source_enc = self.tokenizer(\n",
        "            source_text,\n",
        "            max_length=self.max_source_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Tokenize the target\n",
        "        target_enc = self.tokenizer(\n",
        "            target_text,\n",
        "            max_length=self.max_target_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': source_enc['input_ids'].squeeze(),\n",
        "            'attention_mask': source_enc['attention_mask'].squeeze(),\n",
        "            'labels': target_enc['input_ids'].squeeze()\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYvqfiXZGHem"
      },
      "source": [
        "## 4. Fine-Tune T5 on Our Combined Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfj20BL6GHen"
      },
      "source": [
        "# 1) Load T5 tokenizer + model\n",
        "model_name = \"t5-base\"  # could use 't5-small' for faster training or 't5-large' for better capacity\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# 2) Create dataset\n",
        "jsonl_path = \"/content/drive/MyDrive/MinorProject/unified_mcq_dataset.jsonl\"\n",
        "dataset = MCQDataset(jsonl_path, tokenizer)\n",
        "print(\"Total samples:\", len(dataset))\n",
        "\n",
        "# 3) Split into train/val\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "print(\"Train size:\", len(train_dataset), \"Validation size:\", len(val_dataset))\n",
        "\n",
        "# 4) Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/MinorProject/t5_mcq_finetuned\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=200,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/content/drive/MyDrive/MinorProject/logs\",\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# 5) Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# 6) Train!\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlsSKixKGHen"
      },
      "source": [
        "## 5. Save the Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H4dgF1JGHen"
      },
      "source": [
        "# After training finishes, save the model & tokenizer\n",
        "trainer.save_model(\"/content/drive/MyDrive/MinorProject/t5_mcq_finetuned\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/MinorProject/t5_mcq_finetuned\")\n",
        "print(\"Model saved in '/content/drive/MyDrive/MinorProject/t5_mcq_finetuned' folder.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "omW1Xt1BG01e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xtuotp4tG0wF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjqF7_vYGHeo"
      },
      "source": [
        "## 6. Classical Distractor Pipeline\n",
        "We now define all the functions for **question generation** (rule-based) and **distractor** generation, **filtering**, and **re-ranking**.\n",
        "However, remember: Our T5 model will produce text in the format:\n",
        "```\n",
        "\"Question: <question> Answer: <answer>\"\n",
        "```\n",
        "In the final step, we can combine these approaches so we can do either:\n",
        "1. Let T5 produce a question + short answer, then generate distractors.\n",
        "2. Or directly do the classical approach on each sentence (the code below shows how)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sl4jWs0GHeo"
      },
      "source": [
        "# Utility for text cleanup & sentence splitting\n",
        "def clean_text(text: str) -> str:\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = text.replace(\"–\", \"-\")\n",
        "    return text\n",
        "\n",
        "def split_into_sentences(text: str):\n",
        "    text = clean_text(text)\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def pick_top_sentences(sentences, num=5):\n",
        "    \"\"\" Simple heuristic: pick the longest sentences first. \"\"\"\n",
        "    sorted_sents = sorted(sentences, key=lambda x: len(x), reverse=True)\n",
        "    return sorted_sents[:num]\n",
        "\n",
        "# ---- Basic Rule-based Q Generation (Optional) ----\n",
        "def generate_question_from_sentence(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    target_span = None\n",
        "    answer_text = None\n",
        "    question_word = \"What\"  # default\n",
        "\n",
        "    ent_label_to_qw = {\n",
        "        \"PERSON\": \"Who\",\n",
        "        \"GPE\": \"Where\",\n",
        "        \"LOC\": \"Where\",\n",
        "        \"ORG\": \"What\",\n",
        "        \"DATE\": \"When\",\n",
        "        \"TIME\": \"When\",\n",
        "    }\n",
        "\n",
        "    # 1) Named entities\n",
        "    for ent in doc.ents:\n",
        "        answer_text = ent.text\n",
        "        if ent.label_ in ent_label_to_qw:\n",
        "            question_word = ent_label_to_qw[ent.label_]\n",
        "        target_span = ent\n",
        "        break\n",
        "\n",
        "    # 2) Fallback to noun chunks\n",
        "    if not target_span:\n",
        "        for chunk in doc.noun_chunks:\n",
        "            if len(chunk.text) > 2:\n",
        "                target_span = chunk\n",
        "                answer_text = chunk.text\n",
        "                question_word = \"What\"\n",
        "                break\n",
        "\n",
        "    if not target_span:\n",
        "        return None\n",
        "\n",
        "    # Replace chunk with question word\n",
        "    start = target_span.start_char\n",
        "    end = target_span.end_char\n",
        "    question_sentence = sentence[:start] + question_word + sentence[end:]\n",
        "    question_sentence = question_sentence.strip()\n",
        "    if not question_sentence.endswith(\"?\"):\n",
        "        question_sentence = question_sentence.rstrip(\".\") + \"?\"\n",
        "\n",
        "    return (question_sentence, answer_text.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWk2lhzEGHep"
      },
      "source": [
        "# Distractor Generation\n",
        "# 1) Time phrases\n",
        "def is_time_phrase(text):\n",
        "    pattern = r\"(early|late|mid)\\\\s+(1\\\\d\\\\d0s|2\\\\d\\\\d0s)\"\n",
        "    return bool(re.search(pattern, text.lower()))\n",
        "\n",
        "def generate_time_distractors(time_text, num_distractors=3):\n",
        "    pattern = r\"(early|late|mid)\\\\s+(\\\\d{4})s\"\n",
        "    match = re.search(pattern, time_text.lower())\n",
        "    if not match:\n",
        "        return [\"in the early 2000s\", \"in the mid 1990s\", \"in the late 1980s\"][:num_distractors]\n",
        "\n",
        "    descriptor = match.group(1)\n",
        "    decade_str = match.group(2)\n",
        "    try:\n",
        "        decade_int = int(decade_str)\n",
        "    except:\n",
        "        decade_int = 1990\n",
        "\n",
        "    descriptors = [\"early\", \"mid\", \"late\"]\n",
        "    shifts = [-1, 0, 1, 2, -2]\n",
        "    candidates = []\n",
        "    for desc in descriptors:\n",
        "        for shift in shifts:\n",
        "            new_decade = decade_int + (shift * 10)\n",
        "            if desc == descriptor and new_decade == decade_int:\n",
        "                continue\n",
        "            cand = f\"in the {desc} {new_decade}s\"\n",
        "            candidates.append(cand)\n",
        "\n",
        "    random.shuffle(candidates)\n",
        "    return candidates[:num_distractors]\n",
        "\n",
        "# 2) ConceptNet\n",
        "def get_conceptnet_candidates(word, language=\"en\", limit=50):\n",
        "    url = f\"http://api.conceptnet.io/c/{language}/{word}?limit={limit}\"\n",
        "    try:\n",
        "        resp = requests.get(url)\n",
        "        if resp.status_code != 200:\n",
        "            return []\n",
        "        data = resp.json()\n",
        "        candidates = []\n",
        "        for edge in data.get(\"edges\", []):\n",
        "            for node in [edge.get(\"start\", {}), edge.get(\"end\", {})]:\n",
        "                term = node.get(\"term\", \"\")\n",
        "                parts = term.split(\"/\")\n",
        "                if len(parts) >= 4:\n",
        "                    candidate = parts[3].replace(\"_\", \" \").strip()\n",
        "                    if candidate.lower() != word.lower():\n",
        "                        candidates.append(candidate)\n",
        "        return list(set(candidates))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# 3) WordNet\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def wordnet_candidates(word, pos=wn.NOUN):\n",
        "    distractors = set()\n",
        "    synsets = wn.synsets(word, pos=pos)\n",
        "    if not synsets:\n",
        "        return []\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            lw = lemma.name().replace(\"_\", \" \")\n",
        "            if lw.lower() != word.lower():\n",
        "                distractors.add(lw)\n",
        "\n",
        "    for syn in synsets:\n",
        "        for hyper in syn.hypernyms():\n",
        "            for hypo in hyper.hyponyms():\n",
        "                for lemma in hypo.lemmas():\n",
        "                    lw = lemma.name().replace(\"_\", \" \")\n",
        "                    if lw.lower() != word.lower():\n",
        "                        distractors.add(lw)\n",
        "    return list(distractors)\n",
        "\n",
        "# 4) Sense2Vec\n",
        "def sense2vec_candidates(tagged_word, topn=15):\n",
        "    if not s2v:\n",
        "        return []\n",
        "    candidates = []\n",
        "    try:\n",
        "        sim_list = s2v.most_similar(tagged_word, n=topn)\n",
        "        for cand, score in sim_list:\n",
        "            cand_word = cand.split(\"|\")[0]\n",
        "            if cand_word.lower() != tagged_word.split(\"|\")[0].lower():\n",
        "                candidates.append(cand_word)\n",
        "    except KeyError:\n",
        "        pass\n",
        "    return list(set(candidates))\n",
        "\n",
        "# 5) Named Entity Distractors (Placeholders)\n",
        "def generate_location_distractors(location_text, num=3):\n",
        "    locs = [\"London\", \"Berlin\", \"Tokyo\", \"Sydney\", \"New York\"]\n",
        "    random.shuffle(locs)\n",
        "    return locs[:num]\n",
        "\n",
        "def generate_person_distractors(person_text, num=3):\n",
        "    ppl = [\"Rihanna\", \"Lady Gaga\", \"Adele\", \"Britney Spears\", \"Taylor Swift\"]\n",
        "    random.shuffle(ppl)\n",
        "    return ppl[:num]\n",
        "\n",
        "# ---- Filtering & Re-ranking ----\n",
        "def is_synonym_or_lemma(candidate, correct_answer):\n",
        "    if candidate.lower() == correct_answer.lower():\n",
        "        return True\n",
        "    cand_syn = wn.synsets(candidate)\n",
        "    ans_syn = wn.synsets(correct_answer)\n",
        "    if not cand_syn or not ans_syn:\n",
        "        return False\n",
        "    for synset in cand_syn:\n",
        "        if synset in ans_syn:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def is_partial_match(candidate, correct_answer):\n",
        "    c_clean = re.sub(r\"\\\\W+\", \"\", candidate.lower())\n",
        "    a_clean = re.sub(r\"\\\\W+\", \"\", correct_answer.lower())\n",
        "    return (c_clean in a_clean) or (a_clean in c_clean)\n",
        "\n",
        "def filter_candidates(candidates, correct_answer, context):\n",
        "    context_lower = context.lower()\n",
        "    filtered = []\n",
        "    for c in candidates:\n",
        "        c_lower = c.lower()\n",
        "        if is_synonym_or_lemma(c, correct_answer):\n",
        "            continue\n",
        "        if is_partial_match(c, correct_answer):\n",
        "            continue\n",
        "        if c_lower in context_lower:\n",
        "            continue\n",
        "        if correct_answer.lower() in c_lower:\n",
        "            continue\n",
        "        filtered.append(c)\n",
        "    return list(set(filtered))\n",
        "\n",
        "def threshold_rerank(\n",
        "    candidates,\n",
        "    correct_answer,\n",
        "    context,\n",
        "    answer_sim_threshold=0.8,\n",
        "    context_sim_threshold=0.3,\n",
        "    top_k=3\n",
        "):\n",
        "    if not candidates:\n",
        "        return []\n",
        "    correct_emb = sbert_model.encode(correct_answer, convert_to_tensor=True)\n",
        "    context_emb = sbert_model.encode(context, convert_to_tensor=True)\n",
        "    candidate_embs = sbert_model.encode(candidates, convert_to_tensor=True)\n",
        "\n",
        "    sim_ans = util.cos_sim(candidate_embs, correct_emb).squeeze(dim=1)\n",
        "    sim_ctx = util.cos_sim(candidate_embs, context_emb).squeeze(dim=1)\n",
        "\n",
        "    results = []\n",
        "    for i, cand in enumerate(candidates):\n",
        "        ans_score = float(sim_ans[i])\n",
        "        ctx_score = float(sim_ctx[i])\n",
        "        if ans_score < answer_sim_threshold and ctx_score > context_sim_threshold:\n",
        "            final_score = ctx_score - ans_score\n",
        "            results.append((cand, final_score))\n",
        "\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [r[0] for r in results[:top_k]]\n",
        "\n",
        "# 6) Final distractor function\n",
        "def spacy_pos_to_wordnet_pos(spacy_pos):\n",
        "    if spacy_pos.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif spacy_pos.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif spacy_pos.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif spacy_pos.startswith('R'):\n",
        "        return wn.ADV\n",
        "    return wn.NOUN\n",
        "\n",
        "def extract_main_token(phrase):\n",
        "    doc = nlp(phrase)\n",
        "    if len(doc) == 1:\n",
        "        return doc[0].text\n",
        "    best_token = doc.root\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "            best_token = token\n",
        "            break\n",
        "    return best_token.text\n",
        "\n",
        "def generate_best_distractors(correct_answer, context, num_distractors=3):\n",
        "    # 1) Time phrase\n",
        "    if is_time_phrase(correct_answer):\n",
        "        return generate_time_distractors(correct_answer, num_distractors)\n",
        "\n",
        "    # 2) Named entity check\n",
        "    doc_ent = nlp(correct_answer)\n",
        "    if doc_ent.ents:\n",
        "        ent = doc_ent.ents[0]\n",
        "        label = ent.label_\n",
        "        if label in [\"GPE\", \"LOC\"]:\n",
        "            return generate_location_distractors(correct_answer, num_distractors)\n",
        "        elif label in [\"PERSON\", \"ORG\"]:\n",
        "            return generate_person_distractors(correct_answer, num_distractors)\n",
        "\n",
        "    # 3) Single word or multiword?\n",
        "    tokens = correct_answer.split()\n",
        "    multiword = (len(tokens) > 1)\n",
        "\n",
        "    if not multiword:\n",
        "        # single\n",
        "        doc2 = nlp(correct_answer)\n",
        "        if doc2 and len(doc2) == 1:\n",
        "            spacy_pos = doc2[0].tag_\n",
        "            wn_pos = spacy_pos_to_wordnet_pos(spacy_pos)\n",
        "        else:\n",
        "            wn_pos = wn.NOUN\n",
        "\n",
        "        cnet_cands = get_conceptnet_candidates(correct_answer)\n",
        "        wn_cands = wordnet_candidates(correct_answer, pos=wn_pos)\n",
        "\n",
        "        # sense2vec\n",
        "        if wn_pos == wn.NOUN:\n",
        "            s2v_tag = f\"{correct_answer}|NOUN\"\n",
        "        elif wn_pos == wn.VERB:\n",
        "            s2v_tag = f\"{correct_answer}|VERB\"\n",
        "        elif wn_pos == wn.ADJ:\n",
        "            s2v_tag = f\"{correct_answer}|ADJ\"\n",
        "        else:\n",
        "            s2v_tag = f\"{correct_answer}|NOUN\"\n",
        "\n",
        "        s2v_cands = sense2vec_candidates(s2v_tag, topn=15)\n",
        "        all_cands = list(set(cnet_cands + wn_cands + s2v_cands))\n",
        "        filtered = filter_candidates(all_cands, correct_answer, context)\n",
        "        final_distractors = threshold_rerank(filtered, correct_answer, context, top_k=num_distractors)\n",
        "        return final_distractors\n",
        "    else:\n",
        "        # multi\n",
        "        main_token = extract_main_token(correct_answer)\n",
        "        doc3 = nlp(main_token)\n",
        "        if doc3:\n",
        "            wn_pos = spacy_pos_to_wordnet_pos(doc3[0].tag_)\n",
        "        else:\n",
        "            wn_pos = wn.NOUN\n",
        "\n",
        "        cnet_cands = get_conceptnet_candidates(correct_answer)\n",
        "        wn_cands = wordnet_candidates(correct_answer, wn_pos)\n",
        "        if wn_pos == wn.NOUN:\n",
        "            s2v_tag = f\"{main_token}|NOUN\"\n",
        "        elif wn_pos == wn.VERB:\n",
        "            s2v_tag = f\"{main_token}|VERB\"\n",
        "        elif wn_pos == wn.ADJ:\n",
        "            s2v_tag = f\"{main_token}|ADJ\"\n",
        "        else:\n",
        "            s2v_tag = f\"{main_token}|NOUN\"\n",
        "\n",
        "        s2v_cands = sense2vec_candidates(s2v_tag, topn=15)\n",
        "        all_cands = list(set(cnet_cands + wn_cands + s2v_cands))\n",
        "        filtered = filter_candidates(all_cands, correct_answer, context)\n",
        "        final_distractors = threshold_rerank(filtered, correct_answer, context, top_k=num_distractors)\n",
        "        return final_distractors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import requests\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sense2vec import Sense2Vec\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load spaCy (for NER and POS)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load SBERT for re-ranking\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load Sense2Vec\n",
        "s2v = Sense2Vec().from_disk(\"s2v_old\")\n",
        "\n",
        "# =====================================================\n",
        "# 1. Utilities: POS Mapping, Time Detection, etc.\n",
        "# =====================================================\n",
        "def spacy_pos_to_wordnet_pos(spacy_pos):\n",
        "    \"\"\"\n",
        "    Convert spaCy POS tag to WordNet POS constant.\n",
        "    Defaults to wn.NOUN if no match.\n",
        "    \"\"\"\n",
        "    if spacy_pos.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif spacy_pos.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif spacy_pos.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif spacy_pos.startswith('R'):\n",
        "        return wn.ADV\n",
        "    return wn.NOUN\n",
        "\n",
        "def is_time_phrase(text):\n",
        "    \"\"\"\n",
        "    Check if text matches a time expression like 'late 1990s', 'early 2000s', etc.\n",
        "    For more robust detection, you could also check spaCy's NER for `DATE`.\n",
        "    \"\"\"\n",
        "    # Example pattern: (early|late|mid) 19XXs or 20XXs, etc.\n",
        "    pattern = r\"(early|late|mid)\\s+(1\\d\\d0s|2\\d\\d0s)\"\n",
        "    return bool(re.search(pattern, text.lower()))\n",
        "\n",
        "def generate_time_distractors(time_text, num_distractors=3):\n",
        "    \"\"\"\n",
        "    Simple approach: parse 'late 1990s' -> descriptor='late', decade=1990,\n",
        "    then generate variations like 'early 1990s', 'mid 2000s', etc.\n",
        "    \"\"\"\n",
        "    pattern = r\"(early|late|mid)\\s+(\\d{4})s\"\n",
        "    match = re.search(pattern, time_text.lower())\n",
        "    if not match:\n",
        "        # If we can't parse it, just return some placeholders\n",
        "        return [\"in the early 2000s\", \"in the late 1980s\", \"in the mid 1970s\"][:num_distractors]\n",
        "\n",
        "    descriptor = match.group(1)  # early/late/mid\n",
        "    decade_str = match.group(2)  # e.g., '1990'\n",
        "    try:\n",
        "        decade_int = int(decade_str)\n",
        "    except ValueError:\n",
        "        decade_int = 1990\n",
        "\n",
        "    descriptors = [\"early\", \"mid\", \"late\"]\n",
        "    possible_shifts = [-1, 0, 1, 2, -2]  # shift decades\n",
        "\n",
        "    candidates = []\n",
        "    for desc in descriptors:\n",
        "        for shift in possible_shifts:\n",
        "            new_decade = decade_int + (shift * 10)\n",
        "            # Skip identical phrase (same descriptor, same decade)\n",
        "            if desc == descriptor and new_decade == decade_int:\n",
        "                continue\n",
        "            cand = f\"in the {desc} {new_decade}s\"\n",
        "            candidates.append(cand)\n",
        "\n",
        "    random.shuffle(candidates)\n",
        "    return candidates[:num_distractors]\n",
        "\n",
        "# =====================================================\n",
        "# 2. Candidate Retrieval from External Sources\n",
        "# =====================================================\n",
        "def get_conceptnet_candidates(word, language=\"en\", limit=50):\n",
        "    \"\"\"\n",
        "    Grab related terms from ConceptNet.\n",
        "    \"\"\"\n",
        "    url = f\"http://api.conceptnet.io/c/{language}/{word}?limit={limit}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        return []\n",
        "    data = response.json()\n",
        "    candidates = []\n",
        "    for edge in data.get(\"edges\", []):\n",
        "        for node in [edge.get(\"start\", {}), edge.get(\"end\", {})]:\n",
        "            term = node.get(\"term\", \"\")\n",
        "            parts = term.split(\"/\")\n",
        "            if len(parts) >= 4:\n",
        "                candidate = parts[3].replace(\"_\", \" \").strip()\n",
        "                if candidate.lower() != word.lower():\n",
        "                    candidates.append(candidate)\n",
        "    return list(set(candidates))\n",
        "\n",
        "def wordnet_candidates(word, pos=wn.NOUN):\n",
        "    \"\"\"\n",
        "    Combine synonyms + hypernyms->hyponyms from WordNet.\n",
        "    \"\"\"\n",
        "    distractors = set()\n",
        "    synsets = wn.synsets(word, pos=pos)\n",
        "    if not synsets:\n",
        "        return list(distractors)\n",
        "\n",
        "    # Synonyms\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            lemma_word = lemma.name().replace(\"_\", \" \")\n",
        "            if lemma_word.lower() != word.lower():\n",
        "                distractors.add(lemma_word)\n",
        "\n",
        "    # Hypernyms -> Hyponyms\n",
        "    for syn in synsets:\n",
        "        for hyper in syn.hypernyms():\n",
        "            for hypo in hyper.hyponyms():\n",
        "                for lemma in hypo.lemmas():\n",
        "                    lemma_word = lemma.name().replace(\"_\", \" \")\n",
        "                    if lemma_word.lower() != word.lower():\n",
        "                        distractors.add(lemma_word)\n",
        "\n",
        "    return list(distractors)\n",
        "\n",
        "def sense2vec_candidates(tagged_word, topn=15):\n",
        "    \"\"\"\n",
        "    If sense2vec has this sense, return top similar terms.\n",
        "    Otherwise, return empty list.\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    try:\n",
        "        sim_list = s2v.most_similar(tagged_word, n=topn)\n",
        "        for candidate, score in sim_list:\n",
        "            candidate_word = candidate.split(\"|\")[0]\n",
        "            if candidate_word.lower() != tagged_word.split(\"|\")[0].lower():\n",
        "                candidates.append(candidate_word)\n",
        "    except KeyError:\n",
        "        # Sense2Vec doesn't have that sense\n",
        "        pass\n",
        "    return list(set(candidates))\n",
        "\n",
        "# =====================================================\n",
        "# 3. Named-Entity Distractors (e.g., for places)\n",
        "# =====================================================\n",
        "def generate_location_distractors(location_text, num_distractors=3):\n",
        "    \"\"\"\n",
        "    Example custom approach for location-based answers.\n",
        "    You might expand this with a bigger dictionary of cities/countries.\n",
        "    \"\"\"\n",
        "    # Basic placeholders:\n",
        "    locations = [\"Los Angeles\", \"London\", \"Sydney\", \"Tokyo\", \"Berlin\"]\n",
        "    random.shuffle(locations)\n",
        "    # Return first num_distractors\n",
        "    return locations[:num_distractors]\n",
        "\n",
        "def generate_person_distractors(person_text, num_distractors=3):\n",
        "    \"\"\"\n",
        "    If the correct answer is a person name, you might supply other relevant names.\n",
        "    This is extremely domain-specific; we do a placeholder here.\n",
        "    \"\"\"\n",
        "    # Example placeholders:\n",
        "    people = [\"Rihanna\", \"Lady Gaga\", \"Adele\", \"Britney Spears\"]\n",
        "    random.shuffle(people)\n",
        "    return people[:num_distractors]\n",
        "\n",
        "# =====================================================\n",
        "# 4. Advanced Filtering\n",
        "# =====================================================\n",
        "def is_synonym_or_lemma(candidate, correct_answer):\n",
        "    \"\"\"\n",
        "    Check if candidate is same or a direct WordNet synonym of correct_answer.\n",
        "    \"\"\"\n",
        "    if candidate.lower() == correct_answer.lower():\n",
        "        return True\n",
        "    cand_synsets = wn.synsets(candidate)\n",
        "    ans_synsets = wn.synsets(correct_answer)\n",
        "    if not cand_synsets or not ans_synsets:\n",
        "        return False\n",
        "    for synset in cand_synsets:\n",
        "        if synset in ans_synsets:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def is_partial_match(candidate, correct_answer):\n",
        "    \"\"\"\n",
        "    e.g. \"photosynthetic\" vs \"photosynthesis\"\n",
        "    \"\"\"\n",
        "    c_clean = re.sub(r'\\W+', '', candidate.lower())\n",
        "    a_clean = re.sub(r'\\W+', '', correct_answer.lower())\n",
        "    return (c_clean in a_clean) or (a_clean in c_clean)\n",
        "\n",
        "def filter_candidates(candidates, correct_answer, context):\n",
        "    \"\"\"\n",
        "    - Remove synonyms or direct matches\n",
        "    - Remove partial matches\n",
        "    - Remove if candidate appears in the context\n",
        "    - Remove multi-word containing correct answer\n",
        "    \"\"\"\n",
        "    context_lower = context.lower()\n",
        "    filtered = []\n",
        "    for c in candidates:\n",
        "        c_lower = c.lower()\n",
        "        if is_synonym_or_lemma(c, correct_answer):\n",
        "            continue\n",
        "        if is_partial_match(c, correct_answer):\n",
        "            continue\n",
        "        if c_lower in context_lower:\n",
        "            continue\n",
        "        if correct_answer.lower() in c_lower:\n",
        "            continue\n",
        "        filtered.append(c)\n",
        "    return list(set(filtered))\n",
        "\n",
        "# =====================================================\n",
        "# 5. SBERT Threshold-based Re-ranking\n",
        "# =====================================================\n",
        "def threshold_rerank(\n",
        "    candidates,\n",
        "    correct_answer,\n",
        "    context,\n",
        "    answer_sim_threshold=0.8,\n",
        "    context_sim_threshold=0.3,\n",
        "    top_k=3\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Encode with SBERT\n",
        "    2) Calculate sim with correct_answer and context\n",
        "    3) Filter by thresholds, then rank by (context_sim - answer_sim)\n",
        "    4) Return top_k\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    correct_emb = sbert_model.encode(correct_answer, convert_to_tensor=True)\n",
        "    context_emb = sbert_model.encode(context, convert_to_tensor=True)\n",
        "    candidate_embs = sbert_model.encode(candidates, convert_to_tensor=True)\n",
        "\n",
        "    sim_ans = util.cos_sim(candidate_embs, correct_emb).squeeze(dim=1)\n",
        "    sim_ctx = util.cos_sim(candidate_embs, context_emb).squeeze(dim=1)\n",
        "\n",
        "    results = []\n",
        "    for i, cand in enumerate(candidates):\n",
        "        ans_score = float(sim_ans[i])\n",
        "        ctx_score = float(sim_ctx[i])\n",
        "\n",
        "        if ans_score < answer_sim_threshold and ctx_score > context_sim_threshold:\n",
        "            final_score = ctx_score - ans_score\n",
        "            results.append((cand, final_score))\n",
        "\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [r[0] for r in results[:top_k]]\n",
        "\n",
        "# =====================================================\n",
        "# 6. Main \"Best\" Distractor Generation\n",
        "# =====================================================\n",
        "def extract_main_token(phrase):\n",
        "    \"\"\"\n",
        "    For multiword phrase, pick the syntactic head or first NOUN\n",
        "    as the best single token for Sense2Vec fallback.\n",
        "    \"\"\"\n",
        "    doc = nlp(phrase)\n",
        "    if len(doc) == 1:\n",
        "        return doc[0].text  # single word anyway\n",
        "\n",
        "    best_token = doc.root\n",
        "    # or pick first noun if available\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "            best_token = token\n",
        "            break\n",
        "    return best_token.text\n",
        "\n",
        "def generate_best_distractors(correct_answer, context, num_distractors=3):\n",
        "    \"\"\"\n",
        "    Unified approach:\n",
        "    1) Check if single-word. If yes, do normal WordNet+ConceptNet+Sense2Vec.\n",
        "    2) If multiword:\n",
        "       a) If time expression -> do time distractors\n",
        "       b) If named entity -> e.g. location or person distractors\n",
        "       c) Else fallback single-token approach for Sense2Vec\n",
        "    3) Combine with WordNet+ConceptNet using original phrase\n",
        "    4) Filter & Re-rank\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = correct_answer.strip().split()\n",
        "    multiword = (len(tokens) > 1)\n",
        "\n",
        "    # -------------- SINGLE-WORD CASE --------------\n",
        "    if not multiword:\n",
        "        # Get POS\n",
        "        doc = nlp(correct_answer)\n",
        "        wordnet_pos = wn.NOUN\n",
        "        if doc and len(doc) == 1:\n",
        "            spacy_pos = doc[0].tag_\n",
        "            wordnet_pos = spacy_pos_to_wordnet_pos(spacy_pos)\n",
        "\n",
        "        # Gather from WordNet, ConceptNet\n",
        "        cnet_cands = get_conceptnet_candidates(correct_answer)\n",
        "        wn_cands = wordnet_candidates(correct_answer, pos=wordnet_pos)\n",
        "\n",
        "        # Sense2Vec (build tag)\n",
        "        s2v_tag = None\n",
        "        if wordnet_pos == wn.NOUN:\n",
        "            s2v_tag = f\"{correct_answer}|NOUN\"\n",
        "        elif wordnet_pos == wn.VERB:\n",
        "            s2v_tag = f\"{correct_answer}|VERB\"\n",
        "        elif wordnet_pos == wn.ADJ:\n",
        "            s2v_tag = f\"{correct_answer}|ADJ\"\n",
        "        else:\n",
        "            s2v_tag = f\"{correct_answer}|NOUN\"\n",
        "\n",
        "        s2v_cands = sense2vec_candidates(s2v_tag, topn=15)\n",
        "\n",
        "        all_cands = list(set(cnet_cands + wn_cands + s2v_cands))\n",
        "        filtered = filter_candidates(all_cands, correct_answer, context)\n",
        "        final_distractors = threshold_rerank(filtered, correct_answer, context, top_k=num_distractors)\n",
        "        return final_distractors\n",
        "\n",
        "    # -------------- MULTIWORD CASE --------------\n",
        "    # a) Check if time expression\n",
        "    if is_time_phrase(correct_answer):\n",
        "        return generate_time_distractors(correct_answer, num_distractors)\n",
        "\n",
        "    # b) Named Entity check with spaCy\n",
        "    doc = nlp(correct_answer)\n",
        "    if doc.ents and len(doc.ents) > 0:\n",
        "        ent = doc.ents[0]\n",
        "        label = ent.label_\n",
        "        if label in [\"GPE\", \"LOC\"]:\n",
        "            # location fallback\n",
        "            distractors = generate_location_distractors(correct_answer, num_distractors)\n",
        "            return distractors\n",
        "        elif label in [\"PERSON\", \"ORG\"]:\n",
        "            # person fallback\n",
        "            distractors = generate_person_distractors(correct_answer, num_distractors)\n",
        "            return distractors\n",
        "        # else, fallback to single token approach\n",
        "\n",
        "    # c) Fallback single-token approach for Sense2Vec\n",
        "    main_token = extract_main_token(correct_answer)\n",
        "    # We'll do WordNet + ConceptNet with the original phrase,\n",
        "    # but sense2vec with main_token.\n",
        "\n",
        "    # WordNet POS for the main_token\n",
        "    tok_doc = nlp(main_token)\n",
        "    if tok_doc:\n",
        "        wordnet_pos = spacy_pos_to_wordnet_pos(tok_doc[0].tag_)\n",
        "    else:\n",
        "        wordnet_pos = wn.NOUN\n",
        "\n",
        "    cnet_cands = get_conceptnet_candidates(correct_answer)  # entire phrase\n",
        "    wn_cands = wordnet_candidates(correct_answer, pos=wordnet_pos)  # might be less relevant for multiword, but we try\n",
        "\n",
        "    # Sense2Vec on the main_token\n",
        "    # We'll guess a tag\n",
        "    if wordnet_pos == wn.NOUN:\n",
        "        s2v_tag = f\"{main_token}|NOUN\"\n",
        "    elif wordnet_pos == wn.VERB:\n",
        "        s2v_tag = f\"{main_token}|VERB\"\n",
        "    elif wordnet_pos == wn.ADJ:\n",
        "        s2v_tag = f\"{main_token}|ADJ\"\n",
        "    else:\n",
        "        s2v_tag = f\"{main_token}|NOUN\"\n",
        "\n",
        "    s2v_cands = sense2vec_candidates(s2v_tag, topn=15)\n",
        "\n",
        "    all_cands = list(set(cnet_cands + wn_cands + s2v_cands))\n",
        "    filtered = filter_candidates(all_cands, correct_answer, context)\n",
        "    final_distractors = threshold_rerank(filtered, correct_answer, context, top_k=num_distractors)\n",
        "    return final_distractors\n",
        "\n",
        "# =====================================================\n",
        "# 7. Example MCQ Pipeline\n",
        "# =====================================================\n",
        "def generate_qa(context):\n",
        "    \"\"\"\n",
        "    Dummy QA for illustration. Replace with your model if needed.\n",
        "    \"\"\"\n",
        "    # We'll simulate a multiword time phrase answer\n",
        "    question = \" Which country is known as the Land of the Rising Sun?\"\n",
        "    correct_answer = \"Japan\"\n",
        "    return question, correct_answer\n",
        "\n",
        "def generate_mcq(context, num_distractors=3):\n",
        "    question, correct_answer = generate_qa(context)\n",
        "    distractors = generate_best_distractors(correct_answer, context, num_distractors)\n",
        "\n",
        "    # If not enough distractors found\n",
        "    if len(distractors) < num_distractors:\n",
        "        distractors += [\"(No more distractors found)\"] * (num_distractors - len(distractors))\n",
        "\n",
        "    options = distractors + [correct_answer]\n",
        "    random.shuffle(options)\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"options\": options,\n",
        "        \"correct_answer\": correct_answer\n",
        "    }\n",
        "\n",
        "# =====================================================\n",
        "# 8. Test / Demo\n",
        "# =====================================================\n",
        "if __name__ == \"__main__\":\n",
        "    context_example = (\n",
        "\"Japan, an island nation in East Asia, is often referred to as the 'Land of the Rising Sun' because its name in Japanese, Nihon (日本), means 'origin of the sun.' This name reflects Japan’s position east of China, where the sun rises earlier.\"\n",
        "\n",
        "    )\n",
        "\n",
        "    mcq = generate_mcq(context_example, num_distractors=3)\n",
        "    print(\"Question:\", mcq[\"question\"])\n",
        "    print(\"Options:\", mcq[\"options\"])\n",
        "    print(\"Correct Answer:\", mcq[\"correct_answer\"])"
      ],
      "metadata": {
        "id": "0d2t5kFhKogH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z93PFKLGHeq"
      },
      "source": [
        "## 7. End-to-End Generation Demo\n",
        "We show **two** approaches:\n",
        "\n",
        "1. **T5 Approach**: Use the fine-tuned T5 to generate `\"Question: ... Answer: ...\"` from a passage. Then parse out the question and answer, generate **distractors** with the classical pipeline, and form an MCQ.\n",
        "\n",
        "2. **Classical Only**: The naive method that picks a chunk from a sentence and forms a question, then uses the same distractor pipeline.\n",
        "\n",
        "### 7.1. T5 Inference + Distractors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHkwk_uGHeq"
      },
      "source": [
        "# Load our fine-tuned model for inference\n",
        "inference_model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/MinorProject/t5_mcq_finetuned\").to(device)\n",
        "inference_tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/MyDrive/MinorProject/t5_mcq_finetuned\")\n",
        "\n",
        "def t5_generate_question_answer(passage, max_length=128):\n",
        "    \"\"\"\n",
        "    Given a passage, feed \"Generate MCQ: <passage>\" to T5.\n",
        "    Return the string: \"Question: ... Answer: ...\".\n",
        "    \"\"\"\n",
        "    prompt = \"Generate MCQ: \" + passage\n",
        "    inputs = inference_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = inference_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "    decoded = inference_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "def parse_t5_output(t5_output):\n",
        "    \"\"\"\n",
        "    T5 typically outputs: \"Question: <q> Answer: <a>\".\n",
        "    This function extracts <q> and <a> using a simple regex or string split.\n",
        "    \"\"\"\n",
        "    # naive parse\n",
        "    question_part = \"\"\n",
        "    answer_part = \"\"\n",
        "\n",
        "    # Attempt a split by 'Question:' and 'Answer:'\n",
        "    # 1) remove leading/trailing spaces\n",
        "    text = t5_output.strip()\n",
        "    # 2) find positions of 'Question:' and 'Answer:'\n",
        "    q_idx = text.lower().find(\"question:\")\n",
        "    a_idx = text.lower().find(\"answer:\")\n",
        "\n",
        "    if q_idx != -1 and a_idx != -1:\n",
        "        question_part = text[q_idx + len(\"question:\"):a_idx].strip()\n",
        "        answer_part = text[a_idx + len(\"answer:\"):].strip()\n",
        "    return question_part, answer_part\n",
        "\n",
        "def generate_mcq_with_t5_and_distractors(passage, num_distractors=3):\n",
        "    \"\"\"\n",
        "    1) Use T5 to get (question, answer) from the passage.\n",
        "    2) Generate distractors for that answer using the classical pipeline.\n",
        "    3) Return the final MCQ.\n",
        "    \"\"\"\n",
        "    t5_output = t5_generate_question_answer(passage)\n",
        "    question, correct_answer = parse_t5_output(t5_output)\n",
        "\n",
        "    # If we fail to parse a question or answer, bail out\n",
        "    if not question or not correct_answer:\n",
        "        return {\n",
        "            \"passage\": passage,\n",
        "            \"question\": \"(Could not parse question)\",\n",
        "            \"options\": [],\n",
        "            \"correct_answer\": \"\"\n",
        "        }\n",
        "\n",
        "    # Generate distractors from the original passage as context\n",
        "    distractors = generate_best_distractors(correct_answer, passage, num_distractors=num_distractors)\n",
        "    if len(distractors) < num_distractors:\n",
        "        while len(distractors) < num_distractors:\n",
        "            distractors.append(\"(No more distractors)\")\n",
        "\n",
        "    options = distractors + [correct_answer]\n",
        "    random.shuffle(options)\n",
        "\n",
        "    mcq = {\n",
        "        \"passage\": passage,\n",
        "        \"question\": question,\n",
        "        \"options\": options,\n",
        "        \"correct_answer\": correct_answer\n",
        "    }\n",
        "    return mcq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmB6uw6aGHer"
      },
      "source": [
        "### 7.2. Classical-Only Approach\n",
        "If you choose **not** to use T5, you can run the naive approach of:\n",
        "- Splitting the text into sentences,\n",
        "- For each sentence, pick a chunk as the answer,\n",
        "- Generate a question by replacing that chunk with a WH-word,\n",
        "- Use the same distractor pipeline.\n",
        "\n",
        "Below is a function to do exactly that, returning multiple MCQs from a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7syRGt-yGHer"
      },
      "source": [
        "def generate_mcqs_from_text_classical(text, num_questions=5):\n",
        "    sentences = split_into_sentences(text)\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    chosen_sents = pick_top_sentences(sentences, num=num_questions * 2)\n",
        "    mcqs = []\n",
        "    for sent in chosen_sents:\n",
        "        qa = generate_question_from_sentence(sent)\n",
        "        if not qa:\n",
        "            continue\n",
        "        question, correct_answer = qa\n",
        "        distractors = generate_best_distractors(correct_answer, sent, num_distractors=3)\n",
        "        if len(distractors) < 3:\n",
        "            while len(distractors) < 3:\n",
        "                distractors.append(\"(No more distractors)\")\n",
        "\n",
        "        options = distractors + [correct_answer]\n",
        "        random.shuffle(options)\n",
        "\n",
        "        mcq = {\n",
        "            \"context_sentence\": sent,\n",
        "            \"question\": question,\n",
        "            \"options\": options,\n",
        "            \"correct_answer\": correct_answer\n",
        "        }\n",
        "        mcqs.append(mcq)\n",
        "        if len(mcqs) >= num_questions:\n",
        "            break\n",
        "    return mcqs\n",
        "\n",
        "def simple_evaluation_demo(mcqs):\n",
        "    \"\"\"\n",
        "    Just print out MCQs for inspection.\n",
        "    \"\"\"\n",
        "    for i, mcq in enumerate(mcqs, start=1):\n",
        "        print(f\"MCQ {i}:\")\n",
        "        if \"context_sentence\" in mcq:\n",
        "            print(\"Context:\", mcq[\"context_sentence\"])\n",
        "        elif \"passage\" in mcq:\n",
        "            print(\"Passage:\", mcq[\"passage\"])\n",
        "        print(\"Q:\", mcq.get(\"question\", \"No question\"))\n",
        "        for idx, opt in enumerate(mcq.get(\"options\", []), start=1):\n",
        "            print(f\"  {idx}) {opt}\")\n",
        "        print(\"Correct:\", mcq.get(\"correct_answer\", \"N/A\"))\n",
        "        print(\"-\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFnJ5ikRGHer"
      },
      "source": [
        "## 8. Final Demonstration\n",
        "We'll show:\n",
        "- Using **T5** to generate Q&A + classical distractors.\n",
        "- Using the **classical** approach alone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Q0pQlAGHes"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sample_text = (\n",
        "        \"Beyoncé Giselle Knowles-Carter is an American singer, songwriter, and actress. \"\n",
        "        \"Born and raised in Houston, Texas, she rose to fame in the late 1990s as the lead singer of the R&B group Destiny's Child. \"\n",
        "        \"Often referred to as 'Queen Bey', Beyoncé is one of the world's best-selling recording artists, having sold over 120 million records worldwide. \"\n",
        "        \"She has won 28 Grammy Awards and is the most-nominated woman in the award's history.\"\n",
        "        \"She loved Taylor Swift.\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== DEMO 1: T5 + Distractors ===\\n\")\n",
        "    # T5 inference + distractors\n",
        "    mcq_result = generate_mcq_with_t5_and_distractors(sample_text, num_distractors=3)\n",
        "    # simple_evaluation_demo([mcq_result])\n",
        "    print(mcq_result)\n",
        "\n",
        "\n",
        "    # print(\"\\n=== DEMO 2: Classical-Only ===\\n\")\n",
        "    # # Classical approach\n",
        "    # classical_mcqs = generate_mcqs_from_text_classical(sample_text, num_questions=3)\n",
        "    # simple_evaluation_demo(classical_mcqs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install spacy rake-nltk nltk sentence-transformers openai\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m nltk.downloader wordnet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QLPd7LxgRn45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader punkt_tab"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vg58uNL6R1bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from rake_nltk import Rake\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import openai  # For GPT-3.5/4 integration (optional)\n",
        "\n",
        "# Load models\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def generate_distractors(context, question, correct_answer, num_distractors=3):\n",
        "    # Step 1: Extract entities and keywords from context\n",
        "    doc = nlp(context)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'PRODUCT', 'NORP', 'PROFESSION']]\n",
        "\n",
        "    # Step 2: Keyword extraction with RAKE\n",
        "    r = Rake()\n",
        "    r.extract_keywords_from_text(context)\n",
        "    keywords = [kw[0] for kw in r.get_ranked_phrases_with_scores()[:5] if kw[0] != correct_answer]\n",
        "\n",
        "    # Step 3: Get semantic neighbors using WordNet\n",
        "    synsets = wn.synsets(correct_answer)\n",
        "    wordnet_distractors = []\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            # Get hyponyms and related terms\n",
        "            hypernyms = [hyp.name().split('.')[0] for hyp in syn.hypernyms()]\n",
        "            similar = [sim.name().split('.')[0] for sim in syn.similar_tos()]\n",
        "            wordnet_distractors.extend(hypernyms + similar)\n",
        "\n",
        "    # Combine all candidates\n",
        "    candidates = list(set(entities + keywords + wordnet_distractors))\n",
        "\n",
        "    # Step 4: Semantic filtering with Sentence-BERT\n",
        "    correct_embedding = sentence_model.encode([correct_answer])\n",
        "    candidate_embeddings = sentence_model.encode(candidates)\n",
        "\n",
        "    similarities = cosine_similarity(correct_embedding, candidate_embeddings)[0]\n",
        "    filtered = [\n",
        "        (cand, sim) for cand, sim in zip(candidates, similarities)\n",
        "        if 0.4 < sim < 0.8  # Tune these thresholds\n",
        "    ]\n",
        "\n",
        "    # Sort by relevance\n",
        "    filtered.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_candidates = [cand for cand, sim in filtered][:num_distractors*2]\n",
        "\n",
        "    # Step 5: LLM Refinement (using GPT-3.5/4 as example)\n",
        "    prompt = f\"\"\"\n",
        "    Generate {num_distractors} plausible MCQ distractors for this question.\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "    Correct Answer: {correct_answer}\n",
        "    Candidate Distractors: {top_candidates}\n",
        "\n",
        "    Rules:\n",
        "    1. Choose/rewrite candidates to be plausible but incorrect\n",
        "    2. Make them grammatically consistent with the question\n",
        "    3. Ensure they relate to the context\n",
        "\n",
        "    Output ONLY comma-separated distractors:\n",
        "    \"\"\"\n",
        "\n",
        "    # Uncomment for OpenAI API usage\n",
        "    \"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    final_distractors = response.choices[0].message.content.split(', ')\n",
        "    \"\"\"\n",
        "\n",
        "    # Fallback if no API access: use top candidates\n",
        "    final_distractors = top_candidates[:num_distractors]\n",
        "\n",
        "    return list(set(final_distractors))[:num_distractors]  # Ensure uniqueness\n",
        "\n",
        "# Example usage\n",
        "context =  \"Beyoncé Giselle Knowles-Carter is an American singer, songwriter, and actress. Born and raised in Houston, Texas, she rose to fame in the late 1990s as the lead singer of the R&B group Destiny's Child. Often referred to as 'Queen Bey', Beyoncé is one of the world's best-selling recording artists, having sold over 120 million records worldwide.She has won 28 Grammy Awards and is the most-nominated woman in the award's history.\"\n",
        "\n",
        "question = \"When did Beyonce start becoming popular?\"\n",
        "correct_answer = \"in the late 1990s\"\n",
        "\n",
        "distractors = generate_distractors(context, question, correct_answer)\n",
        "print(f\"Correct Answer: {correct_answer}\")\n",
        "print(f\"Distractors: {distractors}\")"
      ],
      "metadata": {
        "id": "rer-9aPjQLXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def t5_generate_distractors(context, question, answer):\n",
        "    input_text = f\"generate distractors: {context} {question} {answer}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\", \")"
      ],
      "metadata": {
        "id": "84Dywm5rQMRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context =  \"Beyoncé Giselle Knowles-Carter is an American singer, songwriter, and actress. Born and raised in Houston, Texas, she rose to fame in the late 1990s as the lead singer of the R&B group Destiny's Child. Often referred to as 'Queen Bey', Beyoncé is one of the world's best-selling recording artists, having sold over 120 million records worldwide.She has won 28 Grammy Awards and is the most-nominated woman in the award's history.\"\n",
        "\n",
        "question = \"When did Beyonce start becoming popular?\"\n",
        "correct_answer = \"in the late 1990s\"\n",
        "\n",
        "distractors = t5_generate_distractors(context, question, correct_answer)"
      ],
      "metadata": {
        "id": "jEVY3p59SNU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distractors"
      ],
      "metadata": {
        "id": "SAAPnVkaSYk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1. Fine-tuned T5 Model\n",
        "class DistractorGenerator:\n",
        "    def __init__(self, model_name=\"t5-base\"):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.semantic_filter = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def prepare_data(self, dataset_name=\"sciq\"):\n",
        "        \"\"\"Load and preprocess training data\"\"\"\n",
        "        dataset = load_dataset(dataset_name)\n",
        "\n",
        "        def format_example(example):\n",
        "            input_text = f\"generate distractors: {example['support']} {example['question']} {example['correct_answer']}\"\n",
        "            target_text = \" , \".join(example['distractors'])\n",
        "            return {\"input_text\": input_text, \"target_text\": target_text}\n",
        "\n",
        "        return dataset.map(format_example, batched=False)\n",
        "\n",
        "    def train(self, dataset, output_dir=\"./distractor_t5\"):\n",
        "        # Tokenization\n",
        "        def tokenize_fn(examples):\n",
        "            model_inputs = self.tokenizer(\n",
        "                examples[\"input_text\"],\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "            with self.tokenizer.as_target_tokenizer():\n",
        "                labels = self.tokenizer(\n",
        "                    examples[\"target_text\"],\n",
        "                    max_length=128,\n",
        "                    truncation=True,\n",
        "                    padding=\"max_length\"\n",
        "                )\n",
        "\n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "            return model_inputs\n",
        "\n",
        "        tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            warmup_steps=500,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=100,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=500\n",
        "        )\n",
        "\n",
        "        # Trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset[\"train\"],\n",
        "            eval_dataset=tokenized_dataset[\"validation\"]\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        trainer.save_model(f\"{output_dir}/final_model\")\n",
        "\n",
        "    def generate(self, context, question, correct_answer, num_distractors=3, temperature=0.7):\n",
        "        \"\"\"Generate and filter distractors\"\"\"\n",
        "        # Generate raw candidates\n",
        "        input_text = f\"generate distractors: {context} {question} {correct_answer}\"\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids,\n",
        "            max_length=128,\n",
        "            num_return_sequences=5,\n",
        "            num_beams=5,\n",
        "            temperature=temperature,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        raw_distractors = [self.tokenizer.decode(out, skip_special_tokens=True)\n",
        "                          for out in outputs]\n",
        "\n",
        "        # Split and clean\n",
        "        candidates = list(set([d.strip() for dist in raw_distractors\n",
        "                             for d in dist.split(\",\")]))\n",
        "\n",
        "        # Semantic filtering\n",
        "        return self.filter_distractors(candidates, correct_answer, num_distractors)\n",
        "\n",
        "    def filter_distractors(self, candidates, correct_answer, num_distractors):\n",
        "        \"\"\"Filter using semantic similarity\"\"\"\n",
        "        # Encode all candidates\n",
        "        all_texts = [correct_answer] + candidates\n",
        "        embeddings = self.semantic_filter.encode(all_texts)\n",
        "\n",
        "        # Calculate similarities\n",
        "        correct_emb = embeddings[0:1]\n",
        "        candidate_embs = embeddings[1:]\n",
        "        similarities = cosine_similarity(correct_emb, candidate_embs)[0]\n",
        "\n",
        "        # Filter criteria\n",
        "        filtered = [\n",
        "            (cand, sim) for cand, sim in zip(candidates, similarities)\n",
        "            if 0.3 < sim < 0.8  # Adjust these thresholds\n",
        "        ]\n",
        "\n",
        "        # Sort by optimal similarity\n",
        "        filtered.sort(key=lambda x: -abs(x[1] - 0.5))  # Prefer 0.4-0.6 range\n",
        "\n",
        "        return [cand for cand, _ in filtered[:num_distractors]]\n",
        "\n",
        "# 2. Usage Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize generator\n",
        "    dg = DistractorGenerator()\n",
        "\n",
        "    # Train on SCIQ dataset (example)\n",
        "    # dataset = dg.prepare_data()\n",
        "    # dg.train(dataset)\n",
        "\n",
        "    # Load pre-trained model (after training)\n",
        "    dg.model = T5ForConditionalGeneration.from_pretrained(\"/distractor_t5/final_model\")\n",
        "\n",
        "    # Generate distractors\n",
        "    context = \"Last week I talked with some of my students about what they wanted to do after they graduated...\"\n",
        "    question = \"We can know from the passage that the author works as a_.\"\n",
        "    correct_answer = \"teacher\"\n",
        "\n",
        "    distractors = dg.generate(context, question, correct_answer)\n",
        "\n",
        "    print(f\"Correct Answer: {correct_answer}\")\n",
        "    print(f\"Plausible Distractors: {distractors}\")"
      ],
      "metadata": {
        "id": "lTbGNT8ESZo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgCDx68UTWAQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "name": "MCQ_Generation_With_T5",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}